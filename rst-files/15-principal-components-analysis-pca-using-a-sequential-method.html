
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" lang="English">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Principal components analysis (PCA) using a sequential method &#8212; code-snippets 1.0.0 documentation</title>
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '1.0.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Basic linear regression" href="16-basic-linear-regression.html" />
    <link rel="prev" title="Integrating an initial value problem (multiple ODEs)" href="13-integrating-an-initial-value-problem-multiple-odes.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="principal-components-analysis-pca-using-a-sequential-method">
<h1>Principal components analysis (PCA) using a sequential method<a class="headerlink" href="#principal-components-analysis-pca-using-a-sequential-method" title="Permalink to this headline">Â¶</a></h1>
<p><strong>Author:</strong> kevindunn</p>
<p><strong>Submitted on:</strong> 2011-08-03 13:31:18-07:00</p>
<p>The singular value decomposition is usually presented as the way to calculate the PCA decomposition of a data matrix.</p>
<p>The NIPALS algorithm is a very computationally tractable way of calculating PCA for large data sets, since we only calculate the components we actually need; whereas SVD calculates all components in one go.</p>
<p>The nonlinear iterative partial least squares (NIPALS) method is more informative, as the interpretation of what the loadings and scores really mean becomes apparent when examining the above code.</p>
<p>For example, in step 1 of the <code class="docutils literal"><span class="pre">while</span></code> loop we see the loading, <span class="math">\(p_a\)</span> contains the regression coefficients when regression the score vector, <span class="math">\(t_a\)</span>, onto each column in <span class="math">\(\mathbf{X}\)</span>. So at convergence of the while loop, any columns in <span class="math">\(\mathbf{X}\)</span> that are strongly correlated, will have similar loading values, <span class="math">\(p_a\)</span>, for those columns.</p>
<img alt="../_images/NIPALS-iterations-columns.png" src="../_images/NIPALS-iterations-columns.png" />
<p><em>Still to come</em></p>
<ul class="simple">
<li>Calculating confidence limits for SPE and Hotelling's <span class="math">\(T^2\)</span> to determine which points are likely outliers</li>
</ul>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># Limitations: does not handle missing data</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="c1"># Download the CSV data file from: </span>
<span class="c1"># http://datasets.connectmv.com/info/silicon-wafer-thickness</span>
<span class="n">raw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="s1">&#39;silicon-wafer-thickness.csv&#39;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">,</span> <span class="n">skip_header</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">N</span><span class="p">,</span> <span class="n">K</span> <span class="o">=</span> <span class="n">raw</span><span class="o">.</span><span class="n">shape</span>

<span class="c1"># Preprocessing: mean center and scale the data columns to unit variance</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">raw</span> <span class="o">-</span> <span class="n">raw</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span> <span class="o">/</span> <span class="n">X</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Verify the centering and scaling</span>
<span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>   <span class="c1"># array([ -3.92198351e-17,  -1.74980803e-16, ...</span>
<span class="n">X</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>    <span class="c1"># [ 1.  1.  1.  1.  1.  1.  1.  1.  1.]</span>

<span class="c1"># We are going to calculate only 2 principal components</span>
<span class="n">A</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1"># We could of course use SVD ...</span>
<span class="n">u</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
 
<span class="c1"># Transpose the &quot;v&quot; array from SVD, which contains the loadings, but retain </span>
<span class="c1"># only the first A columns</span>
<span class="n">svd_P</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">T</span><span class="p">[:,</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">A</span><span class="p">)]</span>  
 
<span class="c1"># Compute the scores from the loadings:</span>
<span class="n">svd_T</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">svd_P</span><span class="p">)</span>

<span class="c1"># But what if we really only wanted calculate A=2 components (imagine SVD on</span>
<span class="c1"># a really big data set where N and K &gt;&gt; 1000). This is why will use the NIPALS,</span>
<span class="c1"># nonlinear iterative partial least squares, method.</span>

<span class="n">nipals_T</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">A</span><span class="p">))</span>
<span class="n">nipals_P</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">K</span><span class="p">,</span> <span class="n">A</span><span class="p">))</span>

<span class="n">tolerance</span> <span class="o">=</span> <span class="mf">1E-10</span>
<span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">A</span><span class="p">):</span>
    
    <span class="n">t_a_guess</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">2</span>
    <span class="n">t_a</span> <span class="o">=</span> <span class="n">t_a_guess</span> <span class="o">+</span> <span class="mf">1.0</span>
    <span class="n">itern</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Repeat until the score, t_a, converges, or until a maximum number of </span>
    <span class="c1"># iterations has been reached</span>
    <span class="k">while</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">t_a_guess</span> <span class="o">-</span> <span class="n">t_a</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">tolerance</span> <span class="ow">or</span> <span class="n">itern</span> <span class="o">&lt;</span> <span class="mi">500</span><span class="p">:</span>
         
        <span class="c1"># 0: starting point for convergence checking on next loop</span>
        <span class="n">t_a_guess</span> <span class="o">=</span> <span class="n">t_a</span>

        <span class="c1"># 1: Regress the scores, t_a, onto every column in X; compute the</span>
        <span class="c1">#    regression coefficient and store it in the loadings, p_a</span>
        <span class="c1">#    i.e. p_a = (X&#39; * t_a)/(t_a&#39; * t_a)</span>
        <span class="n">p_a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">t_a</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">t_a</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">t_a</span><span class="p">)</span>


        <span class="c1"># 2: Normalize loadings p_a to unit length</span>
        <span class="n">p_a</span> <span class="o">=</span> <span class="n">p_a</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">p_a</span><span class="p">)</span>

        <span class="c1"># 3: Now regress each row in X onto the loading vector; store the</span>
        <span class="c1">#    regression coefficients in t_a.</span>
        <span class="c1">#    i.e. t_a = X * p_a / (p_a.T * p_a)</span>
        <span class="n">t_a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">p_a</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">p_a</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">p_a</span><span class="p">)</span>

        <span class="n">itern</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1">#  We&#39;ve converged, or reached the limit on the number of iteration</span>
    
    <span class="c1"># Deflate the part of the data in X that we&#39;ve explained with t_a and p_a</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">t_a</span><span class="p">,</span> <span class="n">p_a</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    
    <span class="c1"># Store result before computing the next component</span>
    <span class="n">nipals_T</span><span class="p">[:,</span> <span class="n">a</span><span class="p">]</span> <span class="o">=</span> <span class="n">t_a</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="n">nipals_P</span><span class="p">[:,</span> <span class="n">a</span><span class="p">]</span> <span class="o">=</span> <span class="n">p_a</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    

<span class="c1"># PCA also has two very important outputs we should calculate:</span>

<span class="c1"># The SPE_X, squared prediction error to the X-space is the residual distance </span>
<span class="c1"># from the model to each data point. </span>
<span class="n">SPE_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># And Hotelling&#39;s T2, the directed distance from the model center to</span>
<span class="c1"># each data point. </span>
<span class="n">inv_covariance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">nipals_T</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">nipals_T</span><span class="p">)</span><span class="o">/</span><span class="n">N</span><span class="p">)</span>
<span class="n">Hot_T2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
    <span class="n">Hot_T2</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">nipals_T</span><span class="p">[</span><span class="n">n</span><span class="p">,:],</span> <span class="n">inv_covariance</span><span class="p">),</span> <span class="n">nipals_T</span><span class="p">[</span><span class="n">n</span><span class="p">,:]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

<span class="c1"># You can verify the NIPALS and SVD results are the same:</span>
<span class="c1"># (you may find that the signs have flipped, but this is still correct)</span>
<span class="n">nipals_T</span> <span class="o">/</span> <span class="n">svd_T</span>
<span class="n">nipals_P</span> <span class="o">/</span> <span class="n">svd_P</span>

<span class="c1"># But since PCA is such a visual tool, we should plot the SPE_X and </span>
<span class="c1"># Hotelling&#39;s T2 values</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pylab</span>
<span class="n">pylab</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">SPE_X</span><span class="p">,</span> <span class="s1">&#39;r.-&#39;</span><span class="p">)</span>  <span class="c1"># see how observation 154 is inconsistent</span>
<span class="n">pylab</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Hot_T2</span><span class="p">,</span> <span class="s1">&#39;k.-&#39;</span><span class="p">)</span> <span class="c1"># observations 38, 39,110, and 154 are outliers</span>

<span class="c1"># And we should also plot the scores:</span>
<span class="n">pylab</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">pylab</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">nipals_T</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">nipals_T</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>
<span class="n">pylab</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>

<span class="c1"># Confirm the outliers in the raw data, giving one extra point above and below</span>
<span class="n">raw</span><span class="p">[</span><span class="mi">37</span><span class="p">:</span><span class="mi">41</span><span class="p">,:]</span>
<span class="n">raw</span><span class="p">[</span><span class="mi">109</span><span class="p">:</span><span class="mi">112</span><span class="p">,:]</span>
<span class="n">raw</span><span class="p">[</span><span class="mi">153</span><span class="p">:</span><span class="mi">156</span><span class="p">,:]</span>

<span class="c1"># Next step for you: exclude observation 38, 39, 110 and 154 and </span>
<span class="c1"># rebuild the PCA model. Can you interpret what the loadings, nipals_P, mean?</span>
</pre></div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
      <li>Previous: <a href="13-integrating-an-initial-value-problem-multiple-odes.html" title="previous chapter">Integrating an initial value problem (multiple ODEs)</a></li>
      <li>Next: <a href="16-basic-linear-regression.html" title="next chapter">Basic linear regression</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, Jiayue Li.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.5</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
      |
      <a href="../_sources/rst-files/15-principal-components-analysis-pca-using-a-sequential-method.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>